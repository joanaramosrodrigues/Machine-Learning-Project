{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7aaa18",
   "metadata": {},
   "source": [
    "Students:\n",
    "\n",
    "Joana Rodrigues - 20240603    \n",
    "Maria Francisca - 20240346     \n",
    "Rui Reis - 20240854      \n",
    "Tomás Silva - 20230982     \n",
    "Victor Pita - 20240596        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54ad42",
   "metadata": {},
   "source": [
    "# Project Part 4 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3ddd2",
   "metadata": {},
   "source": [
    "Predicting agreement reached with clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99cdb1",
   "metadata": {},
   "source": [
    "# Index \n",
    "1. [Imports](#imports)   \n",
    "    1.1. [Import Libraries](#importlibraries)    \n",
    "    1.2. [Import Data files](#importfiles)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c20d6",
   "metadata": {},
   "source": [
    "`Accident Date` Injury date of the claim   \n",
    "`Age at Injury Age` of injured worker when the injury occurred.  \n",
    "`Alternative Dispute Resolution` Adjudication processes external to the Board.   \n",
    "`Assembly Date` The date the claim was first assembled.   \n",
    "`Attorney/ Representative` Is the claim being represented by an Attorney?   \n",
    "`Average Weekly Wage ` The wage used to calculate workers’ compensation, disability, or an Paid Leave wage replacement benefits.      \n",
    "`Birth Year` The reported year of birth of the injured worker.   \n",
    "`C-2 Date` Date of receipt of the Employer's Report of Work-Related; Injury/Illness or equivalent (formerly Form C-2).   \n",
    "`C-3 Date` Date Form C-3 (Employee Claim Form) was received.   \n",
    "`Carrier Name`Name of primary insurance provider responsible for providing workers’ compensation coverage to the injured worker’s employer.   \n",
    "`Carrier Type` Type of primary insurance provider responsible for providing workers’ compensation coverage.   \n",
    "`Claim Identifier` Unique identifier for each claim, assigned by WCB.   \n",
    "`County of Injury` Name of the New York County where the injury occurred.   \n",
    "`COVID-19 Indicator` Indication that the claim may be associated with COVID-19.   \n",
    "`District Name` Name of the WCB district office that oversees claims for that region or area of the state.   \n",
    "`First Hearing Date` Date the first hearing was held on a claim at a WCB hearing location. A blank date means the claim has not yet had a hearing held.    \n",
    "`Gender` The reported gender of the injured worker.   \n",
    "`IME-4 Count` Number of IME-4 forms received per claim. The IME-4 form is the “Independent Examiner's Report of Independent Medical Examination” form.   \n",
    "`Industry Code` NAICS code and descriptions are available at: https://www.naics.com/search-naics-codes-by-industry/.   \n",
    "`Industry Code Description` 2-digit NAICS industry code description used to classify businesses according to their economic activity.   \n",
    "`Medical Fee Region` Approximate region where the injured worker would receive medical service.   \n",
    "`OIICS Nature of Injury Description` The OIICS nature of injury codes & descriptions are available at https://www.bls.gov/iif/oiics_manual_2007.pdf.   \n",
    "`WCIO Cause of Injury Code` The WCIO cause of injury codes & descriptions are at https://www.wcio.org/Active%20PNC/WCIO_Cause_Table.pdf   \n",
    "`WCIO Cause of Injury Description` See description of field above.    \n",
    "`WCIO Nature of Injury Code` The WCIO nature of injury are available at https://www.wcio.org/Active%20PNC/WCIO_Nature_Table.pdf   \n",
    "`WCIO Nature of Injury Description`See description of field above.   \n",
    "`WCIO Part Of Body Code` The WCIO part of body codes & descriptions are available athttps://www.wcio.org/Active%20PNC/WCIO_Part_Table.pdf   \n",
    "`WCIO Part Of Body Description` See description of field above.   \n",
    "`Zip Code` The reported ZIP code of the injured worker’s home address.   \n",
    "`Agreement Reached` Binary variable: Yes if there is an agreement without the involvement of the WCB -> unknown at the start of a claim.   \n",
    "`WCB Decision` Multiclass variable: Decision of the WCB relative to the claim: “Accident” means that claim refers to workplace accident, “Occupational Disease” means illness from the workplace. -> requires WCB deliberation so it is unknown at start of claim.    \n",
    "`Claim Injury Type` Main target variable: Deliberation of the WCB relative to benefits awarded to the claim. Numbering indicates severity   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586699a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"imports\">\n",
    "    \n",
    "# 1. Import\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c1046",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"importlibraries\">\n",
    "    \n",
    "## 1.1. Import libraries\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99405347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import pickle\n",
    "import category_encoders as ce\n",
    "from collections import Counter\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad6cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7265ea2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"importfiles\">\n",
    "    \n",
    "## 1.2. Import data files\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14c7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pickle.load(open(\"df_train_cleaned.pkl\", 'rb'))\n",
    "df_test = pickle.load(open(\"df_test_cleaned.pkl\", 'rb'))\n",
    "# features_selected = pickle.load(open(\"all_features.pkl\", 'rb')) # features selected in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683a574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns =[ 'Claim Injury Type'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d2c6a",
   "metadata": {},
   "source": [
    "Separation of target and other variables and encoding of the target.        \n",
    "Labelencoder is used in this case as the 'Claim Injury Type' can be considered ordinal because there are injury types worse than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c8c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Count  Percentage (%)\n",
      "Agreement Reached                        \n",
      "False              541909       95.353801\n",
      "True                26405        4.646199\n"
     ]
    }
   ],
   "source": [
    "df_train['Agreement Reached'].value_counts()\n",
    "value_counts = df_train['Agreement Reached'].value_counts()\n",
    "percentages = df_train['Agreement Reached'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Combine counts and percentages into a DataFrame\n",
    "value_summary = pd.DataFrame({\n",
    "    'Count': value_counts,\n",
    "    'Percentage (%)': percentages})\n",
    "\n",
    "print(value_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17cff911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Agreement Reached'] = df_train['Agreement Reached'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71234977",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['Agreement Reached'])  # Features\n",
    "y = df_train['Agreement Reached']  # Target column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70db75b",
   "metadata": {},
   "source": [
    "### Function that defines the feature types      \n",
    "(Same function as in the previous notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20bb82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_feature_types(df):\n",
    "    # Identifying date features\n",
    "    date_features = [column for column in df.columns if 'Date' in column]\n",
    "    \n",
    "    # Identifying categorical (object) features initially\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Identifying boolean features\n",
    "    boolean_features = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "    \n",
    "    # Identifying numerical features (integers and floats), but excluding those with 'Code', 'County', or 'Carrier' in their name\n",
    "    numerical_features = [\n",
    "        column for column in df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        if 'Code' not in column and 'County' not in column and 'Carrier' not in column and 'Decision' not in column and 'Indicator' not in column  and 'Grouped' not in column\n",
    "    ]\n",
    "    \n",
    "    # Adding features with 'Code', 'County', or 'Carrier' in the name to categorical features, even if they are numerical\n",
    "    categorical_features.extend([\n",
    "        column for column in df.columns if 'Code' in column or 'County' in column or 'Carrier' in column or 'Decision' in column or 'Grouped' in column\n",
    "    ])\n",
    "\n",
    "    # Removing duplicates in case any feature is accidentally added twice\n",
    "    categorical_features = list(set(categorical_features))\n",
    "    \n",
    "    return {\n",
    "        'date_features': date_features,\n",
    "        'numerical_features': numerical_features,\n",
    "        'categorical_features': categorical_features,\n",
    "        'boolean_features': boolean_features\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7ec09",
   "metadata": {},
   "source": [
    "### 3.1.2 Division of features\n",
    "This division is made because different feature selection methods are used for the different types of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feac70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numerical features based on df_train\n",
    "feature_types = identify_feature_types(X)\n",
    "\n",
    "metric_features = feature_types['numerical_features']\n",
    "\n",
    "# Separate into numerical and non-numerical features for training and testing\n",
    "feat_x_train_num = X[metric_features]\n",
    "\n",
    "feat_x_val_num = df_test[metric_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f5a67",
   "metadata": {},
   "source": [
    "### Correlation with target feature\n",
    "This evaluates the correlation between each feature and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3085a326-2af3-4e0c-be40-df4cbffae0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_features(features_df, target_series, threshold=0.2, correlation_threshold=0.8):\n",
    "    # Ensure target_series index aligns with features_df\n",
    "    target_series = pd.Series(target_series, index=features_df.index)\n",
    "\n",
    "    # Calculate correlation between features and target\n",
    "    correlation_matrix = features_df.corrwith(target_series)\n",
    "\n",
    "    # Convert correlation results to a DataFrame\n",
    "    correlation_df = correlation_matrix.reset_index()\n",
    "    correlation_df.columns = ['Feature', 'Correlation_with_Target']\n",
    "\n",
    "    # Filter features based on absolute correlation threshold\n",
    "    relevant_features = correlation_df[correlation_df['Correlation_with_Target'].abs() > threshold]\n",
    "\n",
    "    # Identify features that are highly correlated with each other\n",
    "    correlation_matrix = features_df.corr()\n",
    "\n",
    "    # Create a list to keep track of features to drop\n",
    "    features_to_drop = []\n",
    "\n",
    "    # Iterate over the upper triangle of the correlation matrix to find highly correlated pairs\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "                feature_1 = correlation_matrix.columns[i]\n",
    "                feature_2 = correlation_matrix.columns[j]\n",
    "                \n",
    "                # Remove the feature with the lower correlation to the target\n",
    "                if abs(correlation_matrix[feature_1].corr(target_series)) < abs(correlation_matrix[feature_2].corr(target_series)):\n",
    "                    features_to_drop.append(feature_1)\n",
    "                else:\n",
    "                    features_to_drop.append(feature_2)\n",
    "\n",
    "    # Remove duplicates from the features_to_drop list\n",
    "    features_to_drop = list(set(features_to_drop))\n",
    "\n",
    "    # Remove highly correlated features from the original feature set\n",
    "    final_relevant_features = relevant_features[~relevant_features['Feature'].isin(features_to_drop)]\n",
    "\n",
    "    return relevant_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7b60d",
   "metadata": {},
   "source": [
    "### Function that handles the missing values      \n",
    "For the numerical columns it replaces them with the median due to not having removed the outliers, this value seems a better approach    \n",
    "For the categorical columns we substitute the missing values with the mode. Initially, we thought about substituting the missing values of the categoricals according to the mode for each type of injury but as there are not a lot of missing values in this stage, we went for a simpler approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594ab5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(X_train, X_val, df_test):\n",
    "    # Identify feature types\n",
    "    feature_types = identify_feature_types(X_train)\n",
    "    \n",
    "    # For numerical columns: fill NaN with the median value (to avoid outlier influence)\n",
    "    numerical_features = feature_types['numerical_features']\n",
    "    \n",
    "    for column in numerical_features:\n",
    "        if column in X_train.columns:\n",
    "            # Use the median value of the training set for filling missing values\n",
    "            median_value = X_train[column].median()  # Using median to handle potential outliers\n",
    "            X_train[column] = X_train[column].fillna(median_value)\n",
    "            X_val[column] = X_val[column].fillna(median_value)\n",
    "            df_test[column] = df_test[column].fillna(median_value)\n",
    "\n",
    "    # For categorical columns: fill NaN with the mode (most frequent value)\n",
    "    categorical_features = feature_types['categorical_features']\n",
    "    \n",
    "    for column in categorical_features:\n",
    "        if column in X_train.columns:\n",
    "            # Use the mode value of the training set for filling missing values\n",
    "            mode_value = X_train[column].mode()[0]  # Find the mode (most frequent value) for each categorical column\n",
    "            X_train[column] = X_train[column].fillna(mode_value)\n",
    "            X_val[column] = X_val[column].fillna(mode_value)\n",
    "            df_test[column] = df_test[column].fillna(mode_value)\n",
    "\n",
    "    # print(\"Columns with missing values in X_train:\")\n",
    "    # print(X_train.columns[X_train.isna().any()].tolist())  # Using .any() to check for any NaNs in each column\n",
    "    # print(\"Columns with missing values in X_val:\")\n",
    "    # print(X_val.columns[X_val.isna().any()].tolist())\n",
    "    # print(\"Columns with missing values in df_test:\")\n",
    "    # print(df_test.columns[df_test.isna().any()].tolist())\n",
    "    return X_train, X_val, df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc11ac2",
   "metadata": {},
   "source": [
    "### Scaler\n",
    "The scaler chosen was MinMaxScale because, even though there were some outliers in the data, the transformation made to log features helped reduce this differences and therefore the outliers are not extreme. Furthermore, this scaler also works well with data that does not follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a54c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(X_train, X_val, df_test):\n",
    "    metric_features = [col for col in X_train.columns if pd.api.types.is_numeric_dtype(X_train[col])]\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_val_scaled = X_val.copy()\n",
    "    X_test_scaled = df_test.copy()\n",
    "\n",
    "    X_train_scaled[metric_features] = scaler.fit_transform(X_train[metric_features])\n",
    "    X_val_scaled[metric_features] = scaler.transform(X_val[metric_features])\n",
    "    X_test_scaled[metric_features] = scaler.transform(X_test_scaled[metric_features])\n",
    "\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bab3d",
   "metadata": {},
   "source": [
    "### Evaluation of model\n",
    "This function is to be incorporated to understand if the changes made in the models worked for the better or not.\n",
    "It allows to see if the classes are being well predicted and if not if they are being predicted at all or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f589ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_predictions(y_pred_val, y_val):\n",
    "    # # Calculate performance metrics for training and testing\n",
    "    # train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    # train_f1_macro = f1_score(y_train, y_pred_train, average='macro')\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    test_f1_macro = f1_score(y_val, y_pred_val, average='macro')\n",
    "\n",
    "    # Display results\n",
    "    # print(f\"Accuracy of train: {train_accuracy:.4f}\")\n",
    "    print(f\"Accuracy of test: {test_accuracy:.4f}\")\n",
    "    # print(f\"F1 Macro (Train): {train_f1_macro:.4f}\")\n",
    "    print(f\"\\033[1mF1 Macro (Test)\\033[0m: {test_f1_macro:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Validation Data:\")\n",
    "    print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "    print(\"\\Confusion Matrix for Validation Data:\")\n",
    "    print(confusion_matrix(y_val, y_pred_val))\n",
    "    return test_accuracy, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfa366-aab0-455a-a33f-f54491aaf7b0",
   "metadata": {},
   "source": [
    "### Joint of all preprocessing and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c09b5bd6-67fd-470a-8932-5c99a2207c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train, X_val, X_test, y_train):\n",
    "    # Data preprocessing\n",
    "    handle_missing_values(X_train, X_val, X_test)\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled = scaler(X_train, X_val, X_test)\n",
    "\n",
    "    # Identify feature types\n",
    "    feature_types = identify_feature_types(X_train_scaled)  # Use training data to identify features\n",
    "    num_features = feature_types['numerical_features']\n",
    "    \n",
    "     # Feature selection \n",
    "    relevant_num_features = numeric_features(X_train_scaled[num_features], y_train)['Feature']\n",
    "\n",
    "    # Combine selected numeric and categorical features\n",
    "    selected_features = list(relevant_num_features)\n",
    "    X_train_selected = X_train_scaled[selected_features]\n",
    "    X_val_selected = X_val_scaled[selected_features]\n",
    "    X_test_selected = X_test_scaled[selected_features]\n",
    "\n",
    "    return X_train_selected, X_val_selected,X_test_selected\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26088cc9",
   "metadata": {},
   "source": [
    "### Clustering Function\n",
    "The following function performs clustering on a dataset to predict the target variable 'Agreement Reached' based on clustering results. It uses stratified k-fold cross-validation to split the data, applies preprocessing steps (handling missing values, encoding, and scaling), and then fits one of three clustering models (KMeans, DBSCAN, or Agglomerative Clustering) to the training data. It maps clusters to the majority class labels and evaluates the model's performance during each fold. Finally, it predicts the labels for the test set based on the learned clustering and returns the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6eb828c-91c1-4cfe-b222-33ca053a3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_agreement_supervised(X, y, X_test, clustering_type='kmeans'):\n",
    "    skf = StratifiedKFold(n_splits=3)\n",
    "    timer = []\n",
    "\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        begin = time.perf_counter()\n",
    "        X_train_selected, X_val_selected,X_test_selected = preprocessing(X_train, X_val, X_test, y_train)\n",
    "\n",
    "        # Supervised Clustering\n",
    "        # Separate by class\n",
    "        X_train_class_0 = X_train_selected[y_train == 0]\n",
    "        X_train_class_1 = X_train_selected[y_train == 1]\n",
    "\n",
    "        # Apply clustering separately for each class\n",
    "        if clustering_type == 'kmeans':\n",
    "            clustering_model_class_0 = KMeans(n_clusters=1, random_state=42)\n",
    "            clustering_model_class_1 = KMeans(n_clusters=1, random_state=42)\n",
    "        elif clustering_type == 'gmm':\n",
    "            clustering_model_class_0 = GaussianMixture(n_components=1, random_state=42)\n",
    "            clustering_model_class_1 = GaussianMixture(n_components=1, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"Only 'kmeans' and 'gmm' clustering are supported for supervised clustering.\")\n",
    "\n",
    "        clustering_model_class_0.fit(X_train_class_0)\n",
    "        clustering_model_class_1.fit(X_train_class_1)\n",
    "\n",
    "        # Assign clusters to validation data\n",
    "        if clustering_type == 'gmm':\n",
    "            val_log_prob_0 = clustering_model_class_0.score_samples(X_val_selected)\n",
    "            val_log_prob_1 = clustering_model_class_1.score_samples(X_val_selected)\n",
    "            y_pred_val = np.where(val_log_prob_0 > val_log_prob_1, 0, 1)\n",
    "        else:  # For KMeans\n",
    "            val_distances_0 = clustering_model_class_0.transform(X_val_selected)\n",
    "            val_distances_1 = clustering_model_class_1.transform(X_val_selected)\n",
    "            y_pred_val = np.where(val_distances_0 < val_distances_1, 0, 1)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        timer.append(end - begin)\n",
    "\n",
    "    print(f\"Average fold duration: {np.mean(timer):.2f}s\")\n",
    "    print(\"Clustering completed.\")\n",
    "    test_accuracy, test_f1_macro = evaluate_model_predictions(y_pred_val, y_val)\n",
    "\n",
    "    # Test Predictions\n",
    "    if clustering_type == 'gmm':\n",
    "        test_log_prob_0 = clustering_model_class_0.score_samples(X_test_selected)\n",
    "        test_log_prob_1 = clustering_model_class_1.score_samples(X_test_selected)\n",
    "        y_pred_test = np.where(test_log_prob_0 > test_log_prob_1, 0, 1)\n",
    "    else:  # For KMeans\n",
    "        test_distances_0 = clustering_model_class_0.transform(X_test_selected)\n",
    "        test_distances_1 = clustering_model_class_1.transform(X_test_selected)\n",
    "        y_pred_test = np.where(test_distances_0 < test_distances_1, 0, 1)\n",
    "\n",
    "    return y_pred_test, test_accuracy, test_f1_macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5701505a-dfe2-4a8b-a011-5da1a95a6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fold duration: 10.34s\n",
      "Clustering completed.\n",
      "Accuracy of test: 0.7753\n",
      "\u001b[1mF1 Macro (Test)\u001b[0m: 0.5557\n",
      "\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.78      0.87    180636\n",
      "        True       0.14      0.78      0.24      8802\n",
      "\n",
      "    accuracy                           0.78    189438\n",
      "   macro avg       0.57      0.78      0.56    189438\n",
      "weighted avg       0.95      0.78      0.84    189438\n",
      "\n",
      "\\Confusion Matrix for Validation Data:\n",
      "[[140025  40611]\n",
      " [  1956   6846]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_kmeans, accuracy_kmeans, f1_kmeans = clustering_agreement_supervised(X, y, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e707a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fold duration: 6.63s\n",
      "Clustering completed.\n",
      "Accuracy of test: 0.7290\n",
      "\u001b[1mF1 Macro (Test)\u001b[0m: 0.5364\n",
      "\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.72      0.84    180636\n",
      "        True       0.14      0.91      0.24      8802\n",
      "\n",
      "    accuracy                           0.73    189438\n",
      "   macro avg       0.57      0.81      0.54    189438\n",
      "weighted avg       0.95      0.73      0.81    189438\n",
      "\n",
      "\\Confusion Matrix for Validation Data:\n",
      "[[130105  50531]\n",
      " [   806   7996]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_gmm, accuracy_gmm, f1_gmm = clustering_agreement_supervised(X, y, df_test, clustering_type='gmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a91b416d-dc22-451e-a06a-cb016fbcd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm(X, y, X_test, threshold =0.7):\n",
    "    skf = StratifiedKFold(n_splits=3)\n",
    "    timer = []\n",
    "\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        begin = time.perf_counter()\n",
    "        X_train_selected, X_val_selected,X_test_selected = preprocessing(X_train, X_val, X_test, y_train)\n",
    "        \n",
    "            # GMM clustering\n",
    "        gmm_class_0 = GaussianMixture(n_components=1, random_state=42)\n",
    "        gmm_class_1 = GaussianMixture(n_components=1, random_state=42)\n",
    "\n",
    "        gmm_class_0.fit(X_train_selected[y_train == 0])\n",
    "        gmm_class_1.fit(X_train_selected[y_train == 1])\n",
    "\n",
    "            # Validation probabilities\n",
    "        val_probs_0 = np.exp(gmm_class_0.score_samples(X_val_selected))\n",
    "        val_probs_1 = np.exp(gmm_class_1.score_samples(X_val_selected))\n",
    "        val_probs_total = val_probs_0 + val_probs_1\n",
    "\n",
    "            # Normalize probabilities\n",
    "        val_probs_1_normalized = val_probs_1 / val_probs_total\n",
    "\n",
    "        y_pred_val = np.where(val_probs_1_normalized >= threshold, 1, 0)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        timer.append(end - begin)\n",
    "\n",
    "    test_accuracy, test_f1_macro= evaluate_model_predictions(y_pred_val, y_val)\n",
    "\n",
    "    print(f\"Average fold duration: {np.mean(timer):.2f}s\")\n",
    "    print(\"Clustering completed.\")\n",
    "\n",
    "    # Test Predictions\n",
    "    test_probs_0 = np.exp(gmm_class_0.score_samples(X_test_selected))\n",
    "    test_probs_1 = np.exp(gmm_class_1.score_samples(X_test_selected))\n",
    "    test_probs_total = test_probs_0 + test_probs_1\n",
    "\n",
    "    # Normalize probabilities\n",
    "    test_probs_1_normalized = test_probs_1 / test_probs_total\n",
    "\n",
    "    # Apply threshold\n",
    "    y_pred_test = np.where(test_probs_1_normalized >= threshold, 1, 0)\n",
    "\n",
    "    return y_pred_test, test_accuracy, test_f1_macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21206f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test: 0.7470\n",
      "\u001b[1mF1 Macro (Test)\u001b[0m: 0.5422\n",
      "\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.74      0.85    180636\n",
      "        True       0.14      0.84      0.24      8802\n",
      "\n",
      "    accuracy                           0.75    189438\n",
      "   macro avg       0.56      0.79      0.54    189438\n",
      "weighted avg       0.95      0.75      0.82    189438\n",
      "\n",
      "\\Confusion Matrix for Validation Data:\n",
      "[[134101  46535]\n",
      " [  1395   7407]]\n",
      "Average fold duration: 5.60s\n",
      "Clustering completed.\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_gmm80, accuracy_gmm80, f1_gmm80 = gmm(X, y, df_test, threshold = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42ecff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test: 0.8659\n",
      "\u001b[1mF1 Macro (Test)\u001b[0m: 0.5842\n",
      "\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.89      0.93    180636\n",
      "        True       0.16      0.46      0.24      8802\n",
      "\n",
      "    accuracy                           0.87    189438\n",
      "   macro avg       0.57      0.67      0.58    189438\n",
      "weighted avg       0.93      0.87      0.89    189438\n",
      "\n",
      "\\Confusion Matrix for Validation Data:\n",
      "[[159982  20654]\n",
      " [  4747   4055]]\n",
      "Average fold duration: 6.62s\n",
      "Clustering completed.\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_gmm85, accuracy_gmm85, f1_gmm85 = gmm(X, y, df_test, threshold = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0693d6f-02cd-4115-862c-47b0eb279509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test: 0.9012\n",
      "\u001b[1mF1 Macro (Test)\u001b[0m: 0.5827\n",
      "\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.93      0.95    180636\n",
      "        True       0.17      0.30      0.22      8802\n",
      "\n",
      "    accuracy                           0.90    189438\n",
      "   macro avg       0.57      0.61      0.58    189438\n",
      "weighted avg       0.93      0.90      0.91    189438\n",
      "\n",
      "\\Confusion Matrix for Validation Data:\n",
      "[[168105  12531]\n",
      " [  6189   2613]]\n",
      "Average fold duration: 5.34s\n",
      "Clustering completed.\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_gmm90, accuracy_gmm90, f1_gmm90 = gmm(X, y, df_test, threshold = 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a23f3503-cafd-4e01-810f-b3bc2f1b6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, y, X_test, threshold=0.7):\n",
    "    skf = StratifiedKFold(n_splits=2)\n",
    "    timer = []\n",
    "\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        begin = time.perf_counter()\n",
    "\n",
    "        X_train_selected, X_val_selected,X_test_selected = preprocessing(X_train, X_val, X_test, y_train)\n",
    "\n",
    "        # KMeans clustering\n",
    "        kmeans_class_0 = KMeans(n_clusters=1, random_state=42)\n",
    "        kmeans_class_1 = KMeans(n_clusters=1, random_state=42)\n",
    "\n",
    "        kmeans_class_0.fit(X_train_selected[y_train == 0])\n",
    "        kmeans_class_1.fit(X_train_selected[y_train == 1])\n",
    "\n",
    "        # Validation distances\n",
    "        val_distances_0 = kmeans_class_0.transform(X_val_selected).flatten()\n",
    "        val_distances_1 = kmeans_class_1.transform(X_val_selected).flatten()\n",
    "\n",
    "        # Convert distances to \"probabilities\" (inverse distance)\n",
    "        val_inverse_dist_0 = 1 / (val_distances_0 + 1e-8)\n",
    "        val_inverse_dist_1 = 1 / (val_distances_1 + 1e-8)\n",
    "        val_total_inverse_dist = val_inverse_dist_0 + val_inverse_dist_1\n",
    "\n",
    "        val_probs_1 = val_inverse_dist_1 / val_total_inverse_dist\n",
    "\n",
    "        # Apply threshold\n",
    "        y_pred_val = np.where(val_probs_1 >= threshold, 1, 0)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        timer.append(end - begin)\n",
    "\n",
    "    print(f\"Fold duration: {end - begin:.2f}s\")\n",
    "    test_accuracy, test_f1_macro=evaluate_model_predictions(y_pred_val, y_val)\n",
    "\n",
    "    print(f\"Average fold duration: {np.mean(timer):.2f}s\")\n",
    "    print(\"Clustering completed.\")\n",
    "\n",
    "    # Test Predictions\n",
    "    test_distances_0 = kmeans_class_0.transform(X_test_selected).flatten()\n",
    "    test_distances_1 = kmeans_class_1.transform(X_test_selected).flatten()\n",
    "\n",
    "    # Convert distances to probabilities\n",
    "    test_inverse_dist_0 = 1 / (test_distances_0 + 1e-8)\n",
    "    test_inverse_dist_1 = 1 / (test_distances_1 + 1e-8)\n",
    "    test_total_inverse_dist = test_inverse_dist_0 + test_inverse_dist_1\n",
    "\n",
    "    test_probs_1 = test_inverse_dist_1 / test_total_inverse_dist\n",
    "\n",
    "    # Apply threshold\n",
    "    y_pred_test = np.where(test_probs_1 >= threshold, 1, 0)\n",
    "\n",
    "    return y_pred_test, test_accuracy, test_f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02630bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold duration: 4.33s\n",
      "Accuracy of test: 0.8476\n",
      "\u001b[1mF1 Macro (Test)\u001b[0m: 0.5902\n",
      "\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.86      0.92    270954\n",
      "        True       0.17      0.59      0.27     13203\n",
      "\n",
      "    accuracy                           0.85    284157\n",
      "   macro avg       0.57      0.73      0.59    284157\n",
      "weighted avg       0.94      0.85      0.88    284157\n",
      "\n",
      "\\Confusion Matrix for Validation Data:\n",
      "[[233040  37914]\n",
      " [  5381   7822]]\n",
      "Average fold duration: 4.37s\n",
      "Clustering completed.\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_kmeans70, accuracy_kmeans70, f1_kmeans70 = kmeans(X, y, df_test, threshold = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02da46db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold duration: 4.50s\n",
      "Accuracy of test: 0.8479\n",
      "\u001b[1mF1 Macro (Test)\u001b[0m: 0.5904\n",
      "\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.86      0.92    270954\n",
      "        True       0.17      0.59      0.27     13203\n",
      "\n",
      "    accuracy                           0.85    284157\n",
      "   macro avg       0.57      0.73      0.59    284157\n",
      "weighted avg       0.94      0.85      0.88    284157\n",
      "\n",
      "\\Confusion Matrix for Validation Data:\n",
      "[[233116  37838]\n",
      " [  5386   7817]]\n",
      "Average fold duration: 4.37s\n",
      "Clustering completed.\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_kmeans75, accuracy_kmeans75, f1_kmeans75 = kmeans(X, y, df_test, threshold = 0.75)\n",
    "#increasing the threshold more than this will make the true not be seen in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565fbdd7-1024-4b6b-938b-67444bfd69cb",
   "metadata": {},
   "source": [
    "## Clustering model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "369609fd-c465-48b0-b7e9-127a0b199796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test F1 Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GMM_Model</th>\n",
       "      <td>0.729004</td>\n",
       "      <td>0.536370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMM_Model_80</th>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.542244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMM_Model_85</th>\n",
       "      <td>0.865914</td>\n",
       "      <td>0.584231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMM_Model_90</th>\n",
       "      <td>0.901181</td>\n",
       "      <td>0.582749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMeans_Model</th>\n",
       "      <td>0.775299</td>\n",
       "      <td>0.555716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMeans_Model_70</th>\n",
       "      <td>0.847637</td>\n",
       "      <td>0.590215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMeans_Model_75</th>\n",
       "      <td>0.847887</td>\n",
       "      <td>0.590389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Test Accuracy  Test F1 Score\n",
       "Model                                        \n",
       "GMM_Model             0.729004       0.536370\n",
       "GMM_Model_80          0.746988       0.542244\n",
       "GMM_Model_85          0.865914       0.584231\n",
       "GMM_Model_90          0.901181       0.582749\n",
       "KMeans_Model          0.775299       0.555716\n",
       "KMeans_Model_70       0.847637       0.590215\n",
       "KMeans_Model_75       0.847887       0.590389"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = {\n",
    "    'Model': [\n",
    "        'GMM_Model', 'GMM_Model_80', 'GMM_Model_85', 'GMM_Model_90',\n",
    "        'KMeans_Model', 'KMeans_Model_70', 'KMeans_Model_75'\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        accuracy_gmm, accuracy_gmm80, accuracy_gmm85, accuracy_gmm90, accuracy_kmeans, accuracy_kmeans70,\n",
    "        accuracy_kmeans75],\n",
    "  'Test F1 Score': [\n",
    "        f1_gmm, f1_gmm80, f1_gmm85, f1_gmm90, f1_kmeans, f1_kmeans70, f1_kmeans75 \n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame with model results\n",
    "results_models = pd.DataFrame(model_results)\n",
    "\n",
    "# Set the 'Model' column as the index\n",
    "results_models.set_index('Model', inplace=True)\n",
    "\n",
    "# Display the DataFrame with the results\n",
    "results_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e90e29-f0e5-4e79-88a3-c97999208070",
   "metadata": {},
   "source": [
    "To use this variable in the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c48eee20-0cec-4c39-916d-f9bf9e69e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been added to the DataFrame and saved to the pickle file.\n"
     ]
    }
   ],
   "source": [
    "df_test = pickle.load(open(\"df_test_cleaned.pkl\", 'rb'))\n",
    "\n",
    "# Add the prediction column to the DataFrame\n",
    "df_test['Agreement Reached'] = y_pred_test_gmm90\n",
    "\n",
    "# Save the updated DataFrame back to the same pickle file\n",
    "file = open('df_test_cleaned.pkl', 'wb')\n",
    "pickle.dump(df_test, file)\n",
    "file.close()  # Close the file after saving\n",
    "\n",
    "print(\"Predictions have been added to the DataFrame and saved to the pickle file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
